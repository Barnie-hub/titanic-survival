{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b732ee",
   "metadata": {},
   "source": [
    "Model Development and Evaluation\n",
    "\n",
    "Notebook Purpose\n",
    "Build, train, and compare multiple machine learning models to predict Titanic survival.\n",
    "\n",
    "Input\n",
    "- `train_features.csv` - Training data with engineered features\n",
    "- `test_features.csv` - Test data with engineered features\n",
    "- `test_passenger_ids.csv` - Passenger IDs for submission\n",
    " Output\n",
    "- Trained model files (`.pkl`)\n",
    "- Model comparison metrics\n",
    "- Feature importance visualizations\n",
    "- Kaggle submission file\n",
    "\n",
    "Models to Evaluate\n",
    "1. **Logistic Regression** - Baseline linear model\n",
    "2. **Random Forest** - Ensemble of decision trees\n",
    "3. **XGBoost** - Gradient boosting (often wins competitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4939479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model persistence\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50929590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up visualization options\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edd8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67f6ee",
   "metadata": {},
   "source": [
    "Load Feature-Engineered Data\n",
    "\n",
    "Loading from the previous checkpoint - our feature engineering outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b627df09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (891, 35)\n",
      "Test set: (418, 34)\n",
      "\n",
      "Training columns: ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_encoded', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T', 'Deck_Unknown', 'FamilySize', 'IsAlone', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'Age_Child', 'Age_Teenager', 'Age_Young_Adult', 'Age_Adult', 'Age_Senior', 'Fare_Low', 'Fare_Medium', 'Fare_High', 'Fare_Very_High']\n"
     ]
    }
   ],
   "source": [
    "# Load the feature-engineered datasets\n",
    "train_df = pd.read_csv('../data/processed/train_features.csv')\n",
    "test_df = pd.read_csv('../data/processed/test_features.csv')\n",
    "test_ids = pd.read_csv('../data/processed/test_passenger_ids.csv')\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print(f\"\\nTraining columns: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93302828",
   "metadata": {},
   "source": [
    "Prepare Features and Target\n",
    "\n",
    "Why Separate X and y?\n",
    "Machine learning models expect:\n",
    "- **X** (features): The input variables used to make predictions\n",
    "- **y** (target): The outcome we're trying to predict (Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93109e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (891, 34)\n",
      "Target shape: (891,)\n",
      "Target distribution:\n",
      "Survived\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = train_df.drop('Survived', axis=1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Test set (no target - that's what we predict)\n",
    "X_test_final = test_df.copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39564dae",
   "metadata": {},
   "source": [
    "Train-Validation Split\n",
    "\n",
    "Why Split the Training Data?\n",
    "We need to evaluate our model on data it hasn't seen during training. This gives us \n",
    "an honest estimate of how well it will perform on the actual test set.\n",
    "\n",
    "- **Training set (80%)**: Used to train the model\n",
    "- **Validation set (20%)**: Used to evaluate and compare models\n",
    "\n",
    "Stratified Splitting\n",
    "We use stratified splitting to ensure the same proportion of survivors in both \n",
    "train and validation sets. This is important because our classes are imbalanced (~38% survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe80774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (712, 34)\n",
      "Validation set: (179, 34)\n",
      "\n",
      "Training target distribution:\n",
      "Survived\n",
      "0    0.616573\n",
      "1    0.383427\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation target distribution:\n",
      "Survived\n",
      "0    0.614525\n",
      "1    0.385475\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nValidation target distribution:\\n{y_val.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ede8b",
   "metadata": {},
   "source": [
    "Feature Scaling\n",
    "\n",
    "Why Scale Features?\n",
    "Some algorithms (like Logistic Regression) are sensitive to feature scales:\n",
    "- Age ranges from 0-80\n",
    "- Fare ranges from 0-512\n",
    "- Binary features are just 0 or 1\n",
    "\n",
    "**StandardScaler** transforms features to have mean=0 and std=1.\n",
    "\n",
    "Important: Fit on Training Only!\n",
    "We fit the scaler on training data and transform both train and validation. \n",
    "This prevents \"data leakage\" - using information from the validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864260e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling complete!\n",
      "\n",
      "Scaled training data sample (first 3 rows, first 5 columns):\n",
      "       Pclass       Age     SibSp     Parch      Fare\n",
      "692  0.829568 -0.322182 -0.465084 -0.466183  0.513812\n",
      "481 -0.370945  0.053575 -0.465084 -0.466183 -0.662563\n",
      "527 -1.571457  0.805089 -0.465084 -0.466183  3.955399\n"
     ]
    }
   ],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data, transform both\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_final.columns)\n",
    "\n",
    "print(\"Feature scaling complete!\")\n",
    "print(f\"\\nScaled training data sample (first 3 rows, first 5 columns):\")\n",
    "print(X_train_scaled.iloc[:3, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f8a85",
   "metadata": {},
   "source": [
    "Cross-Validation Setup\n",
    "\n",
    "Why Cross-Validation?\n",
    "A single train-validation split can give unstable results depending on which samples \n",
    "end up in which set. Cross-validation gives us a more reliable estimate by:\n",
    "\n",
    "1. Splitting data into K folds\n",
    "2. Training K times, each time using a different fold as validation\n",
    "3. Averaging the results\n",
    "\n",
    "We'll use **5-fold stratified cross-validation** for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191ecd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Helper function to evaluate models\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"\n",
    "    Train model and return comprehensive evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_val, y_pred),\n",
    "        'Precision': precision_score(y_val, y_pred),\n",
    "        'Recall': recall_score(y_val, y_pred),\n",
    "        'F1': f1_score(y_val, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "    \n",
    "    # Cross-validation score (on full training set)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    metrics['CV_Mean'] = cv_scores.mean()\n",
    "    metrics['CV_Std'] = cv_scores.std()\n",
    "    \n",
    "    return metrics, y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb3d6f",
   "metadata": {},
   "source": [
    "Model 1: Logistic Regression (Baseline)\n",
    "\n",
    "Why Start with Logistic Regression?\n",
    "Logistic Regression is the classic baseline for binary classification:\n",
    "- **Simple and interpretable**: Coefficients show feature importance\n",
    "- **Fast to train**: Works well even on larger datasets\n",
    "- **Sets a benchmark**: If complex models don't beat this, they're not worth the complexity\n",
    "\n",
    "How It Works\n",
    "Logistic regression models the probability of survival as:\n",
    "P(Survived=1) = 1 / (1 + e^-(β₀ + β₁x₁ + ... + βₙxₙ))\n",
    "\n",
    "Each coefficient (β) tells us how that feature affects survival odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78193dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "  Accuracy:  0.8492\n",
      "  Precision: 0.8182\n",
      "  Recall:    0.7826\n",
      "  F1 Score:  0.8000\n",
      "  ROC AUC:   0.8684\n",
      "  CV Score:  0.8132 (+/- 0.0329)\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "\n",
    "lr_metrics, lr_pred, lr_proba = evaluate_model(\n",
    "    lr_model, X_train_scaled, y_train, X_val_scaled, y_val, 'Logistic Regression'\n",
    ")\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"  Accuracy:  {lr_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {lr_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {lr_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {lr_metrics['F1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {lr_metrics['ROC_AUC']:.4f}\")\n",
    "print(f\"  CV Score:  {lr_metrics['CV_Mean']:.4f} (+/- {lr_metrics['CV_Std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29705cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features (by coefficient magnitude):\n",
      "     Feature  Coefficient\n",
      " Sex_encoded     0.816813\n",
      "      Pclass    -0.490991\n",
      "    Title_Mr    -0.459007\n",
      "       SibSp    -0.434395\n",
      "  FamilySize    -0.369636\n",
      "Title_Master     0.368865\n",
      "   Title_Mrs     0.356277\n",
      "         Age    -0.252910\n",
      "      Deck_E     0.236296\n",
      "Deck_Unknown    -0.232623\n"
     ]
    }
   ],
   "source": [
    "# Examine logistic regression coefficients\n",
    "lr_coef = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by coefficient magnitude):\")\n",
    "print(lr_coef.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135d305",
   "metadata": {},
   "source": [
    "Model 2: Random Forest\n",
    "\n",
    "Why Random Forest?\n",
    "Random Forest is an ensemble method that addresses decision tree weaknesses:\n",
    "- **Reduces overfitting**: Averages many trees to reduce variance\n",
    "- **Handles non-linear relationships**: Trees can capture complex patterns\n",
    "- **No scaling required**: Tree-based models don't need feature scaling\n",
    "- **Feature importance**: Built-in importance scores\n",
    "\n",
    "How It Works\n",
    "1. Create many decision trees, each trained on a random subset of data and features\n",
    "2. Each tree \"votes\" on the prediction\n",
    "3. Final prediction = majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051d3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "  Accuracy:  0.8101\n",
      "  Precision: 0.7966\n",
      "  Recall:    0.6812\n",
      "  F1 Score:  0.7344\n",
      "  ROC AUC:   0.8437\n",
      "  CV Score:  0.8217 (+/- 0.0320)\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest (using unscaled data - trees don't need scaling)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_metrics, rf_pred, rf_proba = evaluate_model(\n",
    "    rf_model, X_train, y_train, X_val, y_val, 'Random Forest'\n",
    ")\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"  Accuracy:  {rf_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {rf_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {rf_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {rf_metrics['F1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {rf_metrics['ROC_AUC']:.4f}\")\n",
    "print(f\"  CV Score:  {rf_metrics['CV_Mean']:.4f} (+/- {rf_metrics['CV_Std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5e2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features (Random Forest):\n",
      "     Feature  Importance\n",
      "    Title_Mr    0.169468\n",
      " Sex_encoded    0.141159\n",
      "        Fare    0.102660\n",
      "         Age    0.084280\n",
      "      Pclass    0.077119\n",
      "Deck_Unknown    0.052334\n",
      "   Title_Mrs    0.050421\n",
      "  Title_Miss    0.047166\n",
      "  FamilySize    0.046621\n",
      "       SibSp    0.031831\n"
     ]
    }
   ],
   "source": [
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Random Forest):\")\n",
    "print(rf_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f51138",
   "metadata": {},
   "source": [
    "Model 3: XGBoost\n",
    "\n",
    "Why XGBoost?\n",
    "XGBoost (eXtreme Gradient Boosting) is often the winning algorithm in competitions:\n",
    "- **Sequential learning**: Each tree corrects errors from previous trees\n",
    "- **Regularization**: Built-in L1/L2 regularization prevents overfitting\n",
    "- **Handles missing values**: Can learn optimal handling of missing data\n",
    "- **Fast**: Optimized implementation\n",
    "\n",
    "How It Works\n",
    "1. Start with a simple prediction (e.g., overall survival rate)\n",
    "2. Build a tree to predict the errors (residuals)\n",
    "3. Add the tree's predictions to improve the model\n",
    "4. Repeat, focusing on samples that are still being predicted incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bf169a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "  Accuracy:  0.8101\n",
      "  Precision: 0.7692\n",
      "  Recall:    0.7246\n",
      "  F1 Score:  0.7463\n",
      "  ROC AUC:   0.8355\n",
      "  CV Score:  0.8329 (+/- 0.0198)\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb_metrics, xgb_pred, xgb_proba = evaluate_model(\n",
    "    xgb_model, X_train, y_train, X_val, y_val, 'XGBoost'\n",
    ")\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"  Accuracy:  {xgb_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {xgb_metrics['Precision']:.4f}\")\n",
    "print(f\"  Recall:    {xgb_metrics['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {xgb_metrics['F1']:.4f}\")\n",
    "print(f\"  ROC AUC:   {xgb_metrics['ROC_AUC']:.4f}\")\n",
    "print(f\"  CV Score:  {xgb_metrics['CV_Mean']:.4f} (+/- {xgb_metrics['CV_Std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f046f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features (XGBoost):\n",
      "       Feature  Importance\n",
      "   Sex_encoded    0.235900\n",
      "      Title_Mr    0.143569\n",
      "        Pclass    0.059179\n",
      "  Deck_Unknown    0.055967\n",
      "    Title_Rare    0.043332\n",
      "    FamilySize    0.034156\n",
      "        Deck_D    0.033052\n",
      "Fare_Very_High    0.028373\n",
      "        Deck_E    0.027556\n",
      "      Fare_Low    0.022939\n"
     ]
    }
   ],
   "source": [
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (XGBoost):\")\n",
    "print(xgb_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3331d70",
   "metadata": {},
   "source": [
    "Model Comparison\n",
    "\n",
    "Now let's compare all three models side-by-side to determine which performs best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5b7c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "                     Accuracy  Precision  Recall      F1  ROC_AUC  CV_Mean  CV_Std\n",
      "Model                                                                             \n",
      "Logistic Regression    0.8492     0.8182  0.7826  0.8000   0.8684   0.8132  0.0329\n",
      "Random Forest          0.8101     0.7966  0.6812  0.7344   0.8437   0.8217  0.0320\n",
      "XGBoost                0.8101     0.7692  0.7246  0.7463   0.8355   0.8329  0.0198\n"
     ]
    }
   ],
   "source": [
    "# Create comparison DataFrame\n",
    "results = pd.DataFrame([lr_metrics, rf_metrics, xgb_metrics])\n",
    "results = results.set_index('Model')\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(results.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6af4d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Main metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = axes[0].bar(x - width, results.loc['Logistic Regression', metrics_to_plot], width, label='Logistic Regression')\n",
    "bars2 = axes[0].bar(x, results.loc['Random Forest', metrics_to_plot], width, label='Random Forest')\n",
    "bars3 = axes[0].bar(x + width, results.loc['XGBoost', metrics_to_plot], width, label='XGBoost')\n",
    "\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0.6, 1.0)\n",
    "\n",
    "# Plot 2: Cross-validation scores with error bars\n",
    "models = ['Logistic Regression', 'Random Forest', 'XGBoost']\n",
    "cv_means = [results.loc[m, 'CV_Mean'] for m in models]\n",
    "cv_stds = [results.loc[m, 'CV_Std'] for m in models]\n",
    "\n",
    "axes[1].bar(models, cv_means, yerr=cv_stds, capsize=5, color=['steelblue', 'forestgreen', 'coral'])\n",
    "axes[1].set_ylabel('CV Accuracy')\n",
    "axes[1].set_title('Cross-Validation Scores (5-Fold)')\n",
    "axes[1].set_ylim(0.7, 0.9)\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):\n",
    "    axes[1].text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/model_comparison.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cabc2d",
   "metadata": {},
   "source": [
    "Confusion Matrices\n",
    "\n",
    "Understanding the Confusion Matrix\n",
    "A confusion matrix shows us where our model makes mistakes:\n",
    "- **True Positives (TP)**: Correctly predicted survivors\n",
    "- **True Negatives (TN)**: Correctly predicted non-survivors\n",
    "- **False Positives (FP)**: Predicted survival but actually died (Type I error)\n",
    "- **False Negatives (FN)**: Predicted death but actually survived (Type II error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8e0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "predictions = [\n",
    "    ('Logistic Regression', lr_pred),\n",
    "    ('Random Forest', rf_pred),\n",
    "    ('XGBoost', xgb_pred)\n",
    "]\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes, predictions):\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Died', 'Survived'],\n",
    "                yticklabels=['Died', 'Survived'])\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/confusion_matrices.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730e997",
   "metadata": {},
   "source": [
    "ROC Curves\n",
    "\n",
    "Why ROC Curves Matter\n",
    "The ROC (Receiver Operating Characteristic) curve shows the trade-off between:\n",
    "- **True Positive Rate** (Sensitivity): How many survivors did we correctly identify?\n",
    "- **False Positive Rate** (1 - Specificity): How many non-survivors did we incorrectly flag as survivors?\n",
    "\n",
    "**AUC (Area Under Curve)**: A single number summarizing performance. \n",
    " 1.0 = perfect, 0.5 = random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7609948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "probabilities = [\n",
    "    ('Logistic Regression', lr_proba, lr_metrics['ROC_AUC']),\n",
    "    ('Random Forest', rf_proba, rf_metrics['ROC_AUC']),\n",
    "    ('XGBoost', xgb_proba, xgb_metrics['ROC_AUC'])\n",
    "]\n",
    "\n",
    "for name, proba, auc in probabilities:\n",
    "    fpr, tpr, _ = roc_curve(y_val, proba)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves - Model Comparison')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/rocauc_scores.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa7634",
   "metadata": {},
   "source": [
    "Feature Importance Comparison\n",
    "\n",
    "Let's see which features are most important across our models. \n",
    "This helps us understand what drives survival predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07742bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine feature importance from all models\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'LR_Importance': np.abs(lr_model.coef_[0]),  # Use absolute value for comparison\n",
    "    'RF_Importance': rf_model.feature_importances_,\n",
    "    'XGB_Importance': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Normalize to 0-1 scale for comparison\n",
    "for col in ['LR_Importance', 'RF_Importance', 'XGB_Importance']:\n",
    "    importance_df[col] = importance_df[col] / importance_df[col].max()\n",
    "\n",
    "# Calculate average importance\n",
    "importance_df['Avg_Importance'] = importance_df[['LR_Importance', 'RF_Importance', 'XGB_Importance']].mean(axis=1)\n",
    "importance_df = importance_df.sort_values('Avg_Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "top_features = importance_df.head(15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(top_features))\n",
    "width = 0.25\n",
    "\n",
    "ax.barh(x - width, top_features['LR_Importance'], width, label='Logistic Regression')\n",
    "ax.barh(x, top_features['RF_Importance'], width, label='Random Forest')\n",
    "ax.barh(x + width, top_features['XGB_Importance'], width, label='XGBoost')\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.set_xlabel('Normalized Importance')\n",
    "ax.set_title('Top 15 Features by Importance (All Models)')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()  # Most important at top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/feature_importance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58d7f2",
   "metadata": {},
   "source": [
    "Select Best Model\n",
    "\n",
    "Based on our evaluation, let's select the best performing model for final predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae857313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model based on CV accuracy: XGBoost\n",
      "CV Score: 0.8329 (+/- 0.0198)\n"
     ]
    }
   ],
   "source": [
    "# Determine best model based on CV score (most reliable metric)\n",
    "best_model_name = results['CV_Mean'].idxmax()\n",
    "print(f\"Best model based on CV accuracy: {best_model_name}\")\n",
    "print(f\"CV Score: {results.loc[best_model_name, 'CV_Mean']:.4f} (+/- {results.loc[best_model_name, 'CV_Std']:.4f})\")\n",
    "\n",
    "# Select the best model object\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Also determine if we need scaled data\n",
    "use_scaled = best_model_name == 'Logistic Regression'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ae004",
   "metadata": {},
   "source": [
    "Retrain on Full Training Data\n",
    "\n",
    "Now that we've selected our best model, we retrain it on the **full** training dataset \n",
    "(not just the 80% training split) to maximize the information it learns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e8df891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained XGBoost on full training data (891 samples)\n"
     ]
    }
   ],
   "source": [
    "# Retrain best model on full training data\n",
    "if use_scaled:\n",
    "    # Scale full training data\n",
    "    scaler_full = StandardScaler()\n",
    "    X_full_scaled = scaler_full.fit_transform(X)\n",
    "    best_model.fit(X_full_scaled, y)\n",
    "    print(f\"Retrained {best_model_name} on full scaled training data ({len(X)} samples)\")\n",
    "else:\n",
    "    best_model.fit(X, y)\n",
    "    print(f\"Retrained {best_model_name} on full training data ({len(X)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82739c",
   "metadata": {},
   "source": [
    "Generate Test Predictions\n",
    "\n",
    "Now we generate predictions for the test set - these are the passengers whose survival \n",
    "we're trying to predict for the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c71213aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 418 predictions\n",
      "Predicted survival rate: 34.69%\n",
      "(Training survival rate was: 38.38%)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for test set\n",
    "if use_scaled:\n",
    "    X_test_for_pred = scaler_full.transform(X_test_final)\n",
    "else:\n",
    "    X_test_for_pred = X_test_final\n",
    "\n",
    "test_predictions = best_model.predict(X_test_for_pred)\n",
    "\n",
    "print(f\"Generated {len(test_predictions)} predictions\")\n",
    "print(f\"Predicted survival rate: {test_predictions.mean():.2%}\")\n",
    "print(f\"(Training survival rate was: {y.mean():.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venc",
   "language": "python",
   "name": "venc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
